# GID-Project ðŸ—‚
Este projeto Ã© um desafio de configuraÃ§Ã£o de um ambiente adequado para desenvolvimento com os frameworks Apache Airflow e DBT, alÃ©m de criar DAGs e modelos DBT para executar um pipeline de transformaÃ§Ã£o de dados.


# VisÃ£o Geral
O dbt (data build tool) Ã© uma ferramenta de linha de comando que permite que equipes de dados transformem dados em seu warehouse mais eficientemente. O Airflow Ã© uma plataforma para programar e monitorar fluxos de trabalho, permitindo que vocÃª orquestre suas tarefas de transformaÃ§Ã£o de dados (dbt) e outras tarefas relacionadas.

# PrÃ©-requisitos
- Conta na Google Cloud Platform (GCP) com acesso ao BigQuery
- InstalaÃ§Ã£o do dbt configurada para uso com o BigQuery
- InstalaÃ§Ã£o do Apache Airflow
- ConfiguraÃ§Ã£o Inicial
- OBS.: Antes de comeÃ§ar, Ã© necessÃ¡rio ter um projeto dbt configurado em ambiente Linux, no caso deste projeto especÃ­fico foi utilizado o ambiente WSL dentro do Windows.

## IntegraÃ§Ã£o do dbt com o Airflow
    pip install dbt
    pip install apache-airflow
    
## Inicializar o Airflow databse
    airflow db init
  
  ## Inicializar o Airflow webserver e o scheduler
    airflow webserver --port 8080
    airflow scheduler

## Abra a Interface Web do Airflow

Acesse http://localhost:8080 no navegador.

## Acesse as ConexÃµes

Clique em â€œAdminâ€ no menu superior e selecione â€œConexÃµesâ€.

## Adicione uma Nova ConexÃ£o (SE NÃƒO HOUVER)

Clique no botÃ£o â€œCriarâ€ para adicionar uma nova conexÃ£o.

## Preencha os Detalhes da ConexÃ£o

- **Conn Id**: Insira um ID Ãºnico para a conexÃ£o (por exemplo, â€œbigquery_connâ€).
- **Conn Type**: Selecione â€œGoogle BigQueryâ€.
- **Login**: Insira o e-mail da sua conta de serviÃ§o do Google Cloud.
- **Senha**: Insira a chave da sua conta de serviÃ§o do Google Cloud (formato JSON).
  ![image](https://github.com/PedroDPV/GID-Project/assets/103441250/5aa0dfa9-ef1b-4524-a075-7bfe43714dae)

  # Passo 1: Configurar o Projeto dbt
  Certifique-se de que o projeto dbt esteja configurado corretamente, com um arquivo profiles.yml que define como o dbt se conecta ao data warehouse (no caso deste projeto, serÃ¡ o bigquery).
  ##  Execute o comando:
      dbt init projeto_dbt
  ##  Navegue atÃ© a Pasta do Projeto DBT:
      cd projeto_dbt
  ## Configure o profiles.yml:
  O arquivo profiles.yml Ã© onde Ã© feita a definiÃ§Ã£o da conexÃ£o com o seu banco de dados. Normalmente, ele fica localizado em ~/.dbt/.
Ã‰ necessÃ¡rio configurar este arquivo com as informaÃ§Ãµes da sua Service Account do Google Cloud e outras configuraÃ§Ãµes de conexÃ£o com o BigQuery.
Exemplo de configuraÃ§Ã£o para o BigQuery.
  ## yaml:
      projeto_dbt:
       target: dev
       outputs:
        dev:
          type: bigquery
          method: service-account
          project: [SEU_PROJECT_ID]
          dataset: [SEU_DATASET]
          threads: [NUMERO_DE_THREADS]
          keyfile: [CAMINHO_PARA_SUA_SERVICE_ACCOUNT_JSON]
          timeout_seconds: 300
          location: [LOCALIZACAO_DO_DATASET] # ex: US
  ## Teste a conexÃ£o (verificando se o dbt consegue se conectar ao Bigquery):
      dbt debug
  DeverÃ¡ aparecer uma imagem, como a mostrada abaixo:
  
  ![image](https://github.com/PedroDPV/GID-Project/assets/103441250/ab00f79e-d764-4b33-9423-1d89f3c42bc9)

  # Passo 2: Criar Modelos dbt
  - Os modelos dbt definem as transformaÃ§Ãµes SQL que se deseja aplicar aos dados.
  - Crie modelos dbt no diretÃ³rio models do  projeto.

  ## a. Instalar o dbt (se ainda nÃ£o estiver instalado, com o adaptador para BigQuery):
        pip install dbt-bigquery
  ## b. Configurar o Projeto dbt:
  - Navegue atÃ© a pasta onde deseja criar ou jÃ¡ tem o seu projeto dbt:
      ```cd /caminho/para/seu/projeto_dbt```
  - Se ainda nÃ£o tem um projeto dbt, crie um executando:
      ```dbt init seu_nome_de_projeto_dbt```

Isso criarÃ¡ uma nova estrutura de projeto dbt com as configuraÃ§Ãµes iniciais.

![image](https://github.com/PedroDPV/GID-Project/assets/103441250/a2a8bdbc-0600-4feb-a65b-2932db133144)

## c. Criar o Modelo dbt:
   - No diretÃ³rio do seu projeto dbt, navegue atÃ© a pasta models:
        ```cd models```
   - Crie um novo arquivo .sql para o modelo dbt usando um editor de texto como o nano ou vim:
        ```nano cleaned_fakenames.sql```
- Escreva o SQL para o modelo.

```
SELECT
  -- Substituindo strings vazias por valores nulos e capitalizando as palavras
  IF(TRIM(GivenName) = '', NULL, INITCAP(GivenName)) AS GivenName,
  IF(TRIM(Surname) = '', NULL, INITCAP(Surname)) AS Surname,
  -- Traduzindo o gÃªnero de inglÃªs para portuguÃªs
  CASE Gender
    WHEN 'male' THEN 'masculino'
    WHEN 'female' THEN 'feminino'
    ELSE NULL
  END AS Gender,
  -- Capitalizando as cidades e removendo as aspas
  INITCAP(REGEXP_REPLACE(TRIM(City), r'^"|"$', '')) AS City,
  -- Capitalizando os estados e removendo as aspas
  INITCAP(REGEXP_REPLACE(TRIM(StateFull), r'^"|"$', '')) AS StateFull,
  ZipCode,
  EmailAddress,
  Username,
  CCType,
  CCNumber
FROM
  `terraform-366517.dataset_project.tbl_fakenames`
```

## d. Compile e Execute o dbt:

- Compile o cÃ³digo
      ```dbt compile```
- Para rodar o modelo dbt use ```dbt run --models  cleaned_fakenames``` para executar apenas o modelo especÃ­fico no WSL.

![image](https://github.com/PedroDPV/GID-Project/assets/103441250/f97d4d82-526e-4c29-aaaf-9bd592fbcf6d)

  # Passo 4: Criar um DAG do Airflow
  No Airflow, crie uma nova DAG (Directed Acyclic Graph) que definirÃ¡ a sequÃªncia e o agendamento das suas tarefas de transformaÃ§Ã£o de dados.
  Aqui deverÃ£o ser adicionadas tambÃ©m tarefas para executar os comandos dbt, como dbt run.

  ```
  from datetime import datetime, timedelta
from airflow import DAG
from airflow.decorators import task
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
import pandas as pd
import os
import logging
from google.cloud import storage
from google.cloud import bigquery


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

credential_path = '/home/pedrodpv/terraform-366517-733b2d955a83.json'
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path

with DAG('data_transformation_dag', default_args=default_args, schedule_interval='@daily', catchup=False) as dag:

    
    @task
    def extract_data():
        csv_file_path = '~/fakenamgenerator.com_aa22a33f9.csv'
        df = pd.read_csv(csv_file_path)
        df_cleaned = df.dropna()
        df_cleaned = df_cleaned.applymap(lambda s: s.lower() if type(s) == str else s)
        cleaned_csv_file_path = '/home/pedrodpv/cleaned_data.csv'
        df_cleaned.to_csv(cleaned_csv_file_path, index=False)
        return cleaned_csv_file_path

    
    @task
    def load_data(cleaned_csv_file_path):
        logger = logging.getLogger("airflow.task")
        bucket_name = 'gid_bucket_project'
        destination_blob_name = 'cleaned_data.csv'
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)
        blob.upload_from_filename(cleaned_csv_file_path)
        logger.info(f"File {cleaned_csv_file_path} uploaded to {bucket_name}/{destination_blob_name}.")
        return cleaned_csv_file_path

    # Tarefa Python para carregar dados do GCS para o BigQuery
    def load_csv_to_bigquery(bucket_name, source_blob_name, destination_table_id, project_id):
        bigquery_client = bigquery.Client(project=project_id)
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.CSV,
            skip_leading_rows=1,
            autodetect=True,
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        )
        uri = f"gs://{bucket_name}/{source_blob_name}"
        load_job = bigquery_client.load_table_from_uri(
            uri, destination_table_id, job_config=job_config
        )
        load_job.result()
        if load_job.errors is not None:
            raise Exception(f"Erro na job de carregamento: {load_job.errors}")
        print(f"Dados do {uri} carregados para {destination_table_id} com sucesso.")

    load_csv_to_bq_task = PythonOperator(
        task_id='load_csv_to_bq',
        python_callable=load_csv_to_bigquery,
        op_kwargs={
            'bucket_name': 'gid_bucket_project',
            'source_blob_name': 'cleaned_data.csv',
            'destination_table_id': 'terraform-366517.dataset_project.tbl_fakenames',
            'project_id': 'terraform-366517',
        },
        dag=dag,
    )

    
     # Tarefa para executar o modelo 'cleaned_fakenames' no dbt
    dbt_run_task = BashOperator(
    	task_id='dbt_run_cleaned_fakenames',
    	bash_command='cd ~/projeto_dbt && dbt run --models cleaned_fakenames',
    	dag=dag,
   
   )

    cleaned_csv_file_path = extract_data()
    loaded_csv_file_path = load_data(cleaned_csv_file_path)
    loaded_csv_file_path >> load_csv_to_bq_task >> dbt_run_task
  ```

  # Passo 6: Configurar DependÃªncias
  Defina as dependÃªncias entre as tarefas do dbt e outras tarefas no Airflow para garantir que sejam executadas na ordem correta.

  # Passo 7: Executar a DAG
  Ative e execute o seu DAG no Airflow para testar a automaÃ§Ã£o completa do seu pipeline de dados.

  ![image](https://github.com/PedroDPV/GID-Project/assets/103441250/56208b15-0eb0-4a73-80b6-65d162e9d95e)

# O que deve acontecer no final disso tudo?
Um processo no Airflow para rodar o projeto DBT, que aparecerÃ¡, conforme a imagem abaixo, no BigQuery:

![image](https://github.com/PedroDPV/GID-Project/assets/103441250/4633aa9a-f566-4539-9cfd-ad5a4f5c61b1)

![image](https://github.com/PedroDPV/GID-Project/assets/103441250/28134594-fd5b-40a4-9176-714e0615bac3)
![image](https://github.com/PedroDPV/GID-Project/assets/103441250/af0902f1-5439-4e35-83d9-d21728ca05cb)




